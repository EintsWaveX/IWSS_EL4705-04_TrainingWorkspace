{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EintsWaveX/IWSS_EL4705-04_TrainingWorkspace/blob/main/IWSS_EL4705_04_TrainingWorkspace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA4lz-AB3_dL"
      },
      "source": [
        "# **WASTE CLASSIFICATION TRAINING**\n",
        "\n",
        "_*Complete training notebook for ESP32-CAM waste sorter.*_\n",
        "\n",
        "This section is focused on preparing all necessary libraries to support the Tiny CNN Model creation for the Waste Classification Training Dataset, i.e. TensorFlow (v2.10.0), NumPy, MatPlotLib, OpenCV for Python. All necessary data are found in [TrashNet/feyzazkeve](https://www.kaggle.com/datasets/feyzazkefe/trashnet).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZL_it0Pp2y5m"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install numpy matplotlib opencv-python\n",
        "!pip install ai-edge-litert\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import cv2\n",
        "import binascii\n",
        "from ai_edge_litert.interpreter import Interpreter\n",
        "from google.colab import files\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNsBKQ15DHZ"
      },
      "source": [
        "# **STEP 1: UPLOAD THE DATASET**\n",
        "### Create directory structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gai5u8Xl4N2q"
      },
      "outputs": [],
      "source": [
        "!mkdir -p dataset_320x80/train/plastic dataset_320x80/train/paper dataset_320x80/train/metal\n",
        "!mkdir -p dataset_320x80/test/plastic dataset_320x80/test/paper dataset_320x80/test/metal\n",
        "!mkdir -p dataset_340x60/train/plastic dataset_340x60/train/paper dataset_340x60/train/metal\n",
        "!mkdir -p dataset_340x60/test/plastic dataset_340x60/test/paper dataset_340x60/test/metal\n",
        "\n",
        "print(\"Please upload images to the corresponding folders\")\n",
        "print(\"Required: 320/340 images per class for training, 80/60 for testing\")\n",
        "print(\"Dataset required:\\n\\tPlastic, Paper, and Metal.\\n\")\n",
        "print(\"Following the process below, if no need to re-extract the ZIP file for\")\n",
        "print(\"the dataset training and validation, just press the Cancel Upload button\")\n",
        "print(\"and the program will proceed to whatever in the availabe dataset anyway.\")\n",
        "\n",
        "folders = [\n",
        "    \"dataset_320x80/test/plastic\",\n",
        "    \"dataset_320x80/test/paper\",\n",
        "    \"dataset_320x80/test/metal\",\n",
        "    \"dataset_320x80/train/plastic\",\n",
        "    \"dataset_320x80/train/paper\",\n",
        "    \"dataset_320x80/train/metal\",\n",
        "\n",
        "    \"dataset_340x60/test/plastic\",\n",
        "    \"dataset_340x60/test/paper\",\n",
        "    \"dataset_340x60/test/metal\",\n",
        "    \"dataset_340x60/train/plastic\",\n",
        "    \"dataset_340x60/train/paper\",\n",
        "    \"dataset_340x60/train/metal\"\n",
        "]\n",
        "\n",
        "delete_folders = False\n",
        "reupload_folders = True\n",
        "upload_once = True\n",
        "\n",
        "if delete_folders:\n",
        "    for folder in folders:\n",
        "        if os.path.exists(folder):\n",
        "            for item in os.listdir(folder):\n",
        "                item_path = os.path.join(folder, item)\n",
        "                if os.path.isfile(item_path) or os.path.islink(item_path):\n",
        "                    os.remove(item_path)\n",
        "                elif os.path.isdir(item_path):\n",
        "                    shutil.rmtree(item_path)\n",
        "            print(f\"Cleared contents of: {folder}\")\n",
        "        else:\n",
        "            print(f\"Folder not found: {folder}\")\n",
        "\n",
        "if reupload_folders:\n",
        "    if upload_once:\n",
        "        uploaded = files.upload()\n",
        "        with zipfile.ZipFile(\"IWSS_WasteClassificationDataset.zip\", \"r\") as zip_ref:\n",
        "            zip_ref.extractall(\"\")\n",
        "\n",
        "    source_base = \"IWSS_WasteClassificationDataset\"\n",
        "    classes = [\"metal\", \"plastic\", \"paper\"]\n",
        "    configs = [\n",
        "        (\"dataset_320x80\", 320, 80),\n",
        "        (\"dataset_340x60\", 340, 60)\n",
        "    ]\n",
        "\n",
        "    for dataset_name, train_count, test_count in configs:\n",
        "        for cls in classes:\n",
        "            src_dir = os.path.join(source_base, cls)\n",
        "            files = sorted(os.listdir(src_dir))\n",
        "\n",
        "            for f in files[:train_count]:\n",
        "                shutil.copy(\n",
        "                    os.path.join(src_dir, f),\n",
        "                    f\"{dataset_name}/train/{cls}/{f}\"\n",
        "                )\n",
        "\n",
        "            for f in files[train_count:train_count + test_count]:\n",
        "                shutil.copy(\n",
        "                    os.path.join(src_dir, f),\n",
        "                    f\"{dataset_name}/test/{cls}/{f}\"\n",
        "                )\n",
        "\n",
        "        print(f\"Successfully re-uploaded three {dataset_name} for `train` and `test`.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMVcUkLoJRlQ"
      },
      "source": [
        "# **STEP 2: LOAD AND PREPARE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vv7h7fNyJSye"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE_W, IMG_SIZE_H = 240, 320 # Available: [96, 128, 160, 240] | 320x240 for QVGA\n",
        "BATCH_SIZE = 32\n",
        "DATASET_PATH = \"dataset_320x80\" # Another variation: dataset_340x60\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def augment(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = data_augmentation(image, training=True)\n",
        "    return image, label\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomBrightness(0.15),\n",
        "    tf.keras.layers.RandomContrast(0.2),\n",
        "])\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    f'{DATASET_PATH}/train',\n",
        "    image_size=(IMG_SIZE_W, IMG_SIZE_H),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='int'\n",
        ")\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    f'{DATASET_PATH}/test',\n",
        "    image_size=(IMG_SIZE_W, IMG_SIZE_H),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='int'\n",
        ")\n",
        "\n",
        "train_ds_aug = train_ds.map(\n",
        "    augment,\n",
        "    num_parallel_calls=AUTOTUNE\n",
        ")\n",
        "\n",
        "train_ds_aug = train_ds_aug.cache().prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDA_CrINJsWY"
      },
      "source": [
        "# **STEP 3: CREATE TINY CNN MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_zu8t0uJwVo"
      },
      "outputs": [],
      "source": [
        "# Changes:\n",
        "# 1. Removed the Rescaling(1./255) part, so now the model expects raw pixel\n",
        "#    values in [0, 255].\n",
        "# 2. Revamped the models channels into factors of 16 instead of 8 and/or 12.\n",
        "# 3. Added BatchNormalization, removing Bias from Conv layers, and separating\n",
        "#    ReLU from Conv.\n",
        "# 4. Enhancing the Conv2D layering technique, with the final decision of using\n",
        "#    four blocks in order to get better result in the training and validation\n",
        "#    process.\n",
        "\n",
        "# >>> model = tf.keras.Sequential([\n",
        "# >>>     tf.keras.Input(shape=(IMG_SIZE_W, IMG_SIZE_H, 3)),\n",
        "# >>>     tf.keras.layers.Conv2D(8, 3, padding='same', activation='relu'),\n",
        "# >>>     tf.keras.layers.MaxPooling2D(),\n",
        "# >>>     tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "# >>>     tf.keras.layers.MaxPooling2D(),\n",
        "# >>>     tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "# >>>     tf.keras.layers.GlobalAveragePooling2D(),\n",
        "# >>>     tf.keras.layers.Dense(3, activation='softmax')\n",
        "# >>> ])\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(IMG_SIZE_W, IMG_SIZE_H, 3)),\n",
        "\n",
        "    # ===== Block 1: Edge & color =====\n",
        "    tf.keras.layers.Conv2D(\n",
        "        16, 3, padding='same', use_bias=False\n",
        "    ),\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.9),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "    # ===== Block 2: Texture =====\n",
        "    tf.keras.layers.Conv2D(\n",
        "        32, 3, padding='same', use_bias=False\n",
        "    ),\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.9),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "    # ===== Block 3: Better Abstraction and Class Separation =====\n",
        "    tf.keras.layers.Conv2D(\n",
        "        48, 3, padding='same', use_bias=False\n",
        "    ),\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.9),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "    # ===== Block 4: Material Patterns =====\n",
        "    tf.keras.layers.Conv2D(\n",
        "        64, 3, padding='same', use_bias=False\n",
        "    ),\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.9),\n",
        "    tf.keras.layers.ReLU(),\n",
        "\n",
        "    # ===== Global aggregation =====\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "\n",
        "    # ===== Classification =====\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ablq2bUFKAnH"
      },
      "source": [
        "# **STEP 4: TRAIN THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jp63614lKFwv"
      },
      "outputs": [],
      "source": [
        "# Changes:\n",
        "# 1. Added EarlyStopping in the training process for better model output.\n",
        "# 2. Added unsafe_training flag for complete training set without\n",
        "#    EarlyStopping callback, technically unsafe because underfitting\n",
        "#    or overfitting may occur and some losses may happen along the way.\n",
        "# 3. Added ReduceLROnPlateau.\n",
        "\n",
        "# >>> Epochs Configuration <<<\n",
        "# ✅ Best starting/default range: 30–60 epochs\n",
        "# ❌ Avoid:\n",
        "#           < 20 → likely underfitting\n",
        "#           > 80 → likely overfitting (especially without augmentation)\n",
        "\n",
        "# >>> EPOCHS = 40\n",
        "# >>> history = model.fit(\n",
        "# >>>     train_ds,\n",
        "# >>>     validation_data=val_ds,\n",
        "# >>>     epochs=EPOCHS\n",
        "# >>> )\n",
        "\n",
        "safe_training = True\n",
        "\n",
        "# --- TRAINING PROCESS ---\n",
        "# ...For better tuning, set unsafe_training = False.\n",
        "# ...Configure amount of EPOCHS by indexing.\n",
        "EPOCHS = [30, 40, 50, 60][-1]\n",
        "\n",
        "callbacks, history = None, None\n",
        "# class_weight = {\n",
        "#     0: 1.2,  # Plastic\n",
        "#     1: 1.0,  # Paper\n",
        "#     2: 1.3,  # Metal\n",
        "# }\n",
        "\n",
        "if safe_training:\n",
        "    print(\"[INFO] Safe training is ENABLED!\")\n",
        "    print(\"[INFO] EarlyStopping and ReduceLROnPlateau is APPLIED...\")\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=8,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-5\n",
        "        )\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        train_ds_aug,\n",
        "        # class_weight=class_weight,\n",
        "        validation_data=val_ds,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "else:\n",
        "    print(\"[INFO] Safe training is DISABLED!\")\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds_aug,\n",
        "        # class_weight=class_weight,\n",
        "        validation_data=val_ds,\n",
        "        epochs=EPOCHS,\n",
        "    )\n",
        "\n",
        "# --- TRAINING PROCESS ---\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zZ8nIN0KNil"
      },
      "source": [
        "# **STEP 5: CONVERT TO TFLITE FOR ESP32**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Re5YmEqKQkr"
      },
      "outputs": [],
      "source": [
        "# Changes:\n",
        "# 1. Confirming the representative dataset must be float32 in [0, 255].\n",
        "# 2. Removed Rescaling from previous blocks, ensuring everything in raw pixels.\n",
        "# 3. Fixed ds_taken into taking total amount of training samples over batch\n",
        "#    size.\n",
        "# 4. Removed ds_taken logic, just iterate over train_ds directly.\n",
        "\n",
        "# >>> import math\n",
        "# >>> num_samples = 0\n",
        "# >>> for images, labels in train_ds:\n",
        "# >>>     num_samples += images.shape[0]\n",
        "# >>> ds_taken = 150\n",
        "# >>> ds_taken = math.ceil(num_samples / BATCH_SIZE)\n",
        "\n",
        "def representative_dataset():\n",
        "    # >>> for images, _ in train_ds.take(ds_taken):\n",
        "    # >>>    for i in range(BATCH_SIZE):\n",
        "    # >>>        yield [images[i:i+1]]\n",
        "    for images, _ in train_ds:   # NON-augmented dataset\n",
        "        images = tf.cast(images, tf.float32)\n",
        "        for i in range(images.shape[0]):\n",
        "            yield [images[i:i+1]]   # (1, IMG_SIZE_W, IMG_SIZE_H, 3)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open('waste_classifier.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(f\"Model saved! Size: {len(tflite_model) / 1024:.2f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b5EeUOOKnKR"
      },
      "source": [
        "# **STEP 6: [SAMPLE] TEST THE TFLITE MODEL**\n",
        "\n",
        "### NOTE: Single random sample only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GxPeb-KKp7S"
      },
      "outputs": [],
      "source": [
        "# Changes:\n",
        "# 1. Fixed the Input Preprocess (from ignoring INT8 scaling process before,\n",
        "#    resulting in wrong prediction and such) with the correct INT8 mapping.\n",
        "# 2. Used np.expand_dims instead of list wrapping.\n",
        "# 3. Incorrect output handling produces probabilities instead of INT8 logits,\n",
        "#    thus fixed the output_details part to be more proper in output_float,\n",
        "#    making it is indeed in real-valued logits.\n",
        "# 4. Even though THE model has softmax, INT8 outputs are still quantized.\n",
        "#    Therefore, added changes for the predicted_class and confidence variables\n",
        "#    by introducing new variables exp and probs.\n",
        "# 5. Removed interpreter.get_tensor() logic, can cause confusion and it does\n",
        "#    nothing to the algorithm.\n",
        "# 6. Updated the np.clip() logic inside the INPUT block.\n",
        "\n",
        "interpreter = Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Get one validation sample\n",
        "# --------------------------------------------------\n",
        "for images, labels in val_ds.take(1):\n",
        "    sample_image = images[0].numpy()\n",
        "    sample_label = labels[0].numpy()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "def infer_int8(interpreter, image, input_details, output_details):\n",
        "    # --------------------------------------------------\n",
        "    # INPUT: Quantize float32 -> INT8\n",
        "    # --------------------------------------------------\n",
        "    input_scale, input_zero_point = input_details[0]['quantization']\n",
        "    input_dtype = input_details[0]['dtype']\n",
        "\n",
        "    image_float = image.astype(np.float32)\n",
        "    image_quantized = image_float / input_scale + input_zero_point\n",
        "\n",
        "    info = np.iinfo(input_dtype)\n",
        "    image_int8 = np.clip(\n",
        "        image_quantized, info.min, info.max\n",
        "    ).astype(input_dtype)\n",
        "    # >>> sample_image_int8 = (sample_image * 255).astype(np.int8)\n",
        "\n",
        "    # >>> interpreter.set_tensor(input_details[0]['index'], [sample_image_int8])\n",
        "    interpreter.set_tensor(\n",
        "        input_details[0]['index'],\n",
        "        np.expand_dims(image_int8, axis=0)\n",
        "    )\n",
        "\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # OUTPUT: Dequantize INT8 -> float32 logits\n",
        "    # --------------------------------------------------\n",
        "    output_scale, output_zero_point = output_details[0]['quantization']\n",
        "    output_int8 = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    output_float = output_scale * (\n",
        "        output_int8.astype(np.float32) - output_zero_point\n",
        "    )\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Softmax (approximate confidence)\n",
        "    # --------------------------------------------------\n",
        "    exp = np.exp(output_float[0] - np.max(output_float[0]))\n",
        "    probs = exp / np.sum(exp)\n",
        "\n",
        "    return probs\n",
        "\n",
        "probs = infer_int8(\n",
        "    interpreter,\n",
        "    sample_image,\n",
        "    input_details,\n",
        "    output_details\n",
        ")\n",
        "\n",
        "# >>> confidence = output[0][predicted_class] * 100\n",
        "predicted_class = np.argmax(probs)\n",
        "# >>> predicted_class = np.argmax(output[0])\n",
        "confidence = probs[predicted_class] * 100\n",
        "\n",
        "class_names = ['Plastic', 'Paper', 'Metal']\n",
        "print(\"Test Result:\")\n",
        "print(f\"True      : {class_names[sample_label]}\")\n",
        "print(f\"Predicted : {class_names[predicted_class]}\")\n",
        "print(f\"Confidence: {confidence:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSJT1qe-FMPc"
      },
      "source": [
        "# **STEP 7: [FULL] TEST THE TFLITE MODEL**\n",
        "\n",
        "### NOTE: Entire waste dataset (Plastic, Paper, and Metal)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCu0xtnPFgMh"
      },
      "outputs": [],
      "source": [
        "# ---- STATISTICS ---\n",
        "total = 0\n",
        "correct = 0\n",
        "num_classes = len(class_names)\n",
        "per_class_total = np.zeros(num_classes, dtype=int)\n",
        "per_class_correct = np.zeros(num_classes, dtype=int)\n",
        "\n",
        "# Optional: confusion matrix\n",
        "confusion = np.zeros((num_classes, num_classes), dtype=int)\n",
        "\n",
        "display_all_samples = True\n",
        "\n",
        "# ---- DATASET LOOP ----\n",
        "for images, labels in val_ds:\n",
        "    images = images.numpy()\n",
        "    labels = labels.numpy()\n",
        "\n",
        "    for i in range(images.shape[0]):\n",
        "        probs = infer_int8(\n",
        "            interpreter,\n",
        "            images[i],\n",
        "            input_details,\n",
        "            output_details\n",
        "        )\n",
        "\n",
        "        predicted_class = np.argmax(probs)\n",
        "        confidence = probs[predicted_class] * 100\n",
        "        true_label = labels[i]\n",
        "\n",
        "        # ---- METRICS ----\n",
        "        total += 1\n",
        "        per_class_total[true_label] += 1\n",
        "        confusion[true_label, predicted_class] += 1\n",
        "\n",
        "        if predicted_class == true_label:\n",
        "            correct += 1\n",
        "            per_class_correct[true_label] += 1\n",
        "\n",
        "        if display_all_samples:\n",
        "            print(f\"[True, Predicted, Confidence]: [{class_names[true_label]}, {class_names[predicted_class]}, {confidence:.1f}%]\")\n",
        "\n",
        "# Print all necessary informations.\n",
        "print(f\"\\nOverall Accuracy: {correct / total * 100:.2f}%\\n\")\n",
        "print(\"Per-Class Accuracy:\")\n",
        "for i, name in enumerate(class_names):\n",
        "    acc = per_class_correct[i] / per_class_total[i] * 100\n",
        "    print(f\"{name:8s}: {acc:.2f}% ({per_class_correct[i]}/{per_class_total[i]})\")\n",
        "\n",
        "# ---- CONFUSION MATRIX GRAPHICAL DISPLAY ----\n",
        "# Additional info: Displaying the Confusion Matrix.\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "im = ax.imshow(confusion, cmap=\"Blues\")\n",
        "\n",
        "# Axis labels\n",
        "ax.set_xticks(np.arange(num_classes))\n",
        "ax.set_yticks(np.arange(num_classes))\n",
        "ax.set_xticklabels(class_names)\n",
        "ax.set_yticklabels(class_names)\n",
        "\n",
        "ax.set_xlabel(\"Predicted Label\")\n",
        "ax.set_ylabel(\"True Label\")\n",
        "ax.set_title(\"Confusion Matrix\")\n",
        "\n",
        "# Rotate x-axis labels\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "\n",
        "# Annotate each cell\n",
        "for i in range(num_classes):\n",
        "    for j in range(num_classes):\n",
        "        ax.text(\n",
        "            j, i,\n",
        "            confusion[i, j],\n",
        "            ha=\"center\", va=\"center\",\n",
        "            color=\"white\" if confusion[i, j] > confusion.max() / 2 else \"black\"\n",
        "        )\n",
        "\n",
        "fig.colorbar(im, ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvGFKSFZK0Z4"
      },
      "source": [
        "# **STEP 8: CONVERT TO C ARRAY FOR ESP32**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK1ND9iWK4UL"
      },
      "outputs": [],
      "source": [
        "import binascii\n",
        "from google.colab import files # Re-import files to ensure it's the module\n",
        "\n",
        "def tflite_to_c_array(tflite_model, array_name='waste_model'):\n",
        "    hex_data = binascii.hexlify(tflite_model).decode('utf-8')\n",
        "\n",
        "    c_code = f\"// TensorFlow Lite model for waste classification\\n\"\n",
        "    c_code += f\"// Size: {len(tflite_model)} bytes\\n\\n\"\n",
        "    c_code += f\"const unsigned char {array_name}[] = {{\\n    \"\n",
        "\n",
        "    # Format with 16 bytes per line\n",
        "    for i in range(0, len(hex_data), 2):\n",
        "        if i > 0 and i % 32 == 0:\n",
        "            c_code += \"\\n    \"\n",
        "        c_code += f\"0x{hex_data[i:i+2]}, \"\n",
        "\n",
        "    c_code = c_code[:-2] + \"\\n};\\n\\n\"\n",
        "    c_code += f\"const int {array_name}_len = {len(tflite_model)};\"\n",
        "\n",
        "    return c_code\n",
        "\n",
        "c_array_code = tflite_to_c_array(tflite_model)\n",
        "with open('model.h', 'w') as f:\n",
        "    f.write(c_array_code)\n",
        "\n",
        "print(\"\\nC array saved to 'model.h'\")\n",
        "print(\"Download this file for ESP32: files.download('model.h')\")\n",
        "\n",
        "# Download the files\n",
        "download_flagged = True\n",
        "if download_flagged:\n",
        "    files.download(f'waste_classifier.tflite')\n",
        "    files.download(f'model.h')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOaz4iFziVtIPb8h58UH7U6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}